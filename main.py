import sys
import os

# Streamlit Cloud(Linux)ì—ì„œë§Œ pysqlite3 ì‚¬ìš©
if os.environ.get("STREAMLIT_CLOUD") == "true":
    try:
        __import__("pysqlite3")
        sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")
    except ModuleNotFoundError:
        pass

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_classic.retrievers.multi_query import MultiQueryRetriever
from langchain_classic.chains import RetrievalQA

import streamlit as st
import tempfile
import os
import hashlib

# âœ… Streaming handler (callback)
from langchain_core.callbacks import BaseCallbackHandler

from streamlit_extras.buy_me_a_coffee import button

button(username="bkmAI", floating=True, width=221)

# =================================================
# Streaming Callback Handler
# =================================================
class StreamlitTokenCallbackHandler(BaseCallbackHandler):
    """LLM í† í°ì´ ìƒì„±ë  ë•Œë§ˆë‹¤ Streamlit UIì— ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥"""

    def __init__(self, container):
        self.container = container
        self.text = ""

    def on_llm_start(self, *args, **kwargs):
        self.text = ""
        self.container.markdown("")

    def on_llm_new_token(self, token: str, **kwargs):
        self.text += token
        self.container.markdown(self.text + "â–Œ")

    def on_llm_end(self, *args, **kwargs):
        self.container.markdown(self.text)

# =================================================
# ê¸°ë³¸ ì„¤ì •
# =================================================
load_dotenv()

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PERSIST_DIR = os.path.join(BASE_DIR, "chroma_langchain_db")

# =================================================
# Streamlit UI Title
# =================================================
st.title("ğŸ“„ ChatPDF")
st.write("---")

# =================================================
# OPENAI_API_KEY AI í‚¤ ì…ë ¥ ë°›ê³ ,
# í™˜ê²½ ë³€ìˆ˜ ë“±ë¡, í•˜ìœ„ OpenAI ê´€ë ¨ APIëŠ” ëƒ…ë‘¬ë„ë¨.
# =================================================
openai_key = st.text_input("OPENAI_API_KEY", type="password")

if openai_key:
    os.environ["OPENAI_API_KEY"] = openai_key

# =================================================
# Streamlit UI File Upload
# =================================================
uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"])
st.write("---")

# =================================================
# Utils
# =================================================
def file_hash(uploaded_file) -> str:
    return hashlib.sha256(uploaded_file.getvalue()).hexdigest()

@st.cache_resource
def load_vectorstore(collection_name: str):
    embeddings = OpenAIEmbeddings()
    return Chroma(
        collection_name=collection_name,
        embedding_function=embeddings,
        persist_directory=PERSIST_DIR,
    )

# =================================================
# PDF ì²˜ë¦¬
# =================================================
if uploaded_file is not None:
    file_id = file_hash(uploaded_file)
    collection_name = f"pdf_{file_id[:12]}"

    vector_store = load_vectorstore(collection_name)

    # âœ… ì„ë² ë”© ìŠ¤í”¼ë„ˆ (ìµœì´ˆ 1íšŒë§Œ)
    if vector_store._collection.count() == 0:
        with st.spinner("ğŸ“Œ ì²˜ìŒ ì—…ë¡œë“œëœ PDFì…ë‹ˆë‹¤. ì„ë² ë”© ì¤‘ì…ë‹ˆë‹¤..."):
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_path = os.path.join(temp_dir, uploaded_file.name)
                with open(temp_path, "wb") as f:
                    f.write(uploaded_file.getvalue())

                loader = PyPDFLoader(temp_path)
                documents = loader.load()

                splitter = RecursiveCharacterTextSplitter(
                    chunk_size=300,
                    chunk_overlap=20
                )
                split_docs = splitter.split_documents(documents)

                vector_store.add_documents(split_docs)
                try:
                    vector_store.persist()
                except Exception:
                    pass

        st.success("âœ… ì„ë² ë”© ì™„ë£Œ!")
    else:
        st.success("âœ… ê¸°ì¡´ ë²¡í„° DB ì¬ì‚¬ìš©")

    st.write("---")

    # =================================================
    # Retriever / QA
    # - MultiQueryRetrieverëŠ” ìŠ¤íŠ¸ë¦¬ë° ë„ëŠ” ê²Œ ì•ˆì „(ì¿¼ë¦¬ ìƒì„± í† í°ì´ í™”ë©´ì— ì„ì´ëŠ” ê²ƒ ë°©ì§€)
    # =================================================
    llm_for_queries = ChatOpenAI(temperature=0, max_completion_tokens=512, streaming=False)

    base_retriever = vector_store.as_retriever(search_kwargs={"k": 4})

    mqr = MultiQueryRetriever.from_llm(
        retriever=base_retriever,
        llm=llm_for_queries,
        include_original=True
    )

    # (ìš”ì•½ ë²„íŠ¼ ë“±ì—ì„œ ì“°ëŠ” ê¸°ë³¸ QA: ìŠ¤íŠ¸ë¦¬ë° ì—†ì´)
    qa_non_stream = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(temperature=0, max_completion_tokens=2048, streaming=False),
        retriever=mqr,
        return_source_documents=True
    )

    # ë¬¸ì„œ ìš”ì•½
    if st.button("ğŸ“Œ ë¬¸ì„œ ìš”ì•½"):
        with st.spinner("ğŸ§  ë¬¸ì„œ ìš”ì•½ ìƒì„± ì¤‘..."):
            result = qa_non_stream.invoke({"query": "ì´ ë¬¸ì„œ í•µì‹¬ ìš”ì•½í•´ì¤˜"})
        st.subheader("ğŸ“Œ ìš”ì•½")
        st.write(result["result"])

    st.write("---")

    # PDFì—ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”
    st.subheader("ğŸ¤– PDFì—ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”")
    user_question = st.text_input(
        label="",
        placeholder="PDF ë‚´ìš©ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”",
    )

    if user_question and st.button("ì§ˆë¬¸í•˜ê¸°"):
        # âœ… ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ ì˜ì—­
        stream_box = st.empty()
        handler = StreamlitTokenCallbackHandler(stream_box)

        # âœ… ë‹µë³€ ìƒì„±ìš© LLM: streaming=True + callbacks
        llm_stream = ChatOpenAI(
            temperature=0,
            max_completion_tokens=2048,
            streaming=True,
            callbacks=[handler],
        )

        qa_stream = RetrievalQA.from_chain_type(
            llm=llm_stream,
            retriever=mqr,
            return_source_documents=True
        )

        with st.spinner("ğŸ” ë¬¸ì„œë¥¼ ì°¾ê³  ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
            result = qa_stream.invoke({"query": user_question})

        # ìŠ¤íŠ¸ë¦¬ë°ì´ ì´ë¯¸ ìœ„ì—ì„œ ì¶œë ¥ë˜ì§€ë§Œ, ì™„ë£Œ í›„ í™•ì • ì¶œë ¥ë„ ì›í•˜ë©´ ìœ ì§€
        st.subheader("ğŸ’¬ ë‹µë³€")
        st.write(result["result"])

        st.subheader("ğŸ“ ê·¼ê±° ë¬¸ì„œ (í˜ì´ì§€)")
        for i, doc in enumerate(result["source_documents"], 1):
            st.markdown(f"**[{i}] page {doc.metadata.get('page')}**")
            st.text(doc.page_content[:300])
else:
    st.info("PDFë¥¼ ì—…ë¡œë“œí•˜ë©´ ì§ˆë¬¸ ì…ë ¥ì°½ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.")
